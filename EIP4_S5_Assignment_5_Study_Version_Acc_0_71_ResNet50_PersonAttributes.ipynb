{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EIP4-S5-Assignment 5 Study Version-Acc 0.71-ResNet50-PersonAttributes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilbhatt1/CNN-EIP4-S5/blob/master/EIP4_S5_Assignment_5_Study_Version_Acc_0_71_ResNet50_PersonAttributes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "b58e250a-2177-4e4f-db79-962959d891b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace resized/63.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.resnet50 import ResNet50\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "b8abdb8a-2436-4202-e366-1db11ad942bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "\n",
              "[2 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhtVxsoK3I0J",
        "colab_type": "code",
        "outputId": "359ca8c3-77a5-4176-82ab-1bbb056910fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(df.gender)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufsqxnU73WFX",
        "colab_type": "code",
        "outputId": "0989852c-5d9e-4e0b-8d7d-f0287e7a5128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "pd.get_dummies(df.gender, prefix=\"gender\").head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender_female  gender_male\n",
              "0              0            1\n",
              "1              1            0\n",
              "2              0            1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xHTtKAn3hB-",
        "colab_type": "code",
        "outputId": "a8d4d485-8cad-481a-dc78-72e4543240d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(df['image_path'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvXtUP5L4svT",
        "colab_type": "code",
        "outputId": "056e52eb-96c0-4b00-c249-415b7bc8ef9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(df[['image_path']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEDO83ok40cX",
        "colab_type": "code",
        "outputId": "a0d7ad3a-ca9d-418c-b561-d0051e9a6e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "pd.concat([df[['image_path']],pd.get_dummies(df.gender,prefix='gender')],axis=1).head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  gender_male\n",
              "0  resized/1.jpg              0            1\n",
              "1  resized/2.jpg              1            0\n",
              "2  resized/3.jpg              0            1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "f84b0a95-a718-40cb-c2be-cf03c57e8ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "# df[['image_path']] -> 2 brackets to ensure a dataframe is returned. If it is df['image path'], we will only get a series back\n",
        "# pd.get_dummies will return a dataframe with 1 & 0 for column it is targetting. Check cell above\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head(2).T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0              1\n",
              "image_path                            resized/1.jpg  resized/2.jpg\n",
              "gender_female                                     0              1\n",
              "gender_male                                       1              0\n",
              "imagequality_Average                              1              1\n",
              "imagequality_Bad                                  0              0\n",
              "imagequality_Good                                 0              0\n",
              "age_15-25                                         0              0\n",
              "age_25-35                                         0              0\n",
              "age_35-45                                         1              1\n",
              "age_45-55                                         0              0\n",
              "age_55+                                           0              0\n",
              "weight_normal-healthy                             1              0\n",
              "weight_over-weight                                0              1\n",
              "weight_slightly-overweight                        0              0\n",
              "weight_underweight                                0              0\n",
              "carryingbag_Daily/Office/Work Bag                 0              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1              0\n",
              "carryingbag_None                                  0              1\n",
              "footwear_CantSee                                  0              0\n",
              "footwear_Fancy                                    0              0\n",
              "footwear_Normal                                   1              1\n",
              "emotion_Angry/Serious                             0              1\n",
              "emotion_Happy                                     0              0\n",
              "emotion_Neutral                                   1              0\n",
              "emotion_Sad                                       0              0\n",
              "bodypose_Back                                     0              0\n",
              "bodypose_Front-Frontish                           1              1\n",
              "bodypose_Side                                     0              0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svipont67TZg",
        "colab_type": "code",
        "outputId": "fe77b609-1af1-4d30-84ad-c7ac39b61466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_gender_cols_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gender_female', 'gender_male']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVL86EOH79W9",
        "colab_type": "code",
        "outputId": "98fd59c8-365d-4129-a774-16f1d1b3b9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(_gender_cols_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# List comprehension [col for col in one_hot_df.columns if col.startswith(\"gender\")] helps to select column names starting with 'gender' and\n",
        "# put in a list\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,augmentation=None,incl_orig=False):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "        self.incl_orig = incl_orig\n",
        "\n",
        "# __len__ is a must function we need to give for keras sequence class, which will return number of batches in sequence \n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "# __getitem__ is a must function we need to give for keras sequence class, which will generate one batch of data\n",
        "# index : Denotes index of batch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        " #       print('index',index)\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        " #       print('batch_slice',batch_slice)\n",
        "        items = self.df.iloc[batch_slice]\n",
        " #       print('items:',items)\n",
        "        # 'items' is a df generated from main df based on batch_size. \n",
        "        # items.iterrows() will take item['image_path'] and fetch it to cv2.imread(). '_' means ignore the index we get back from iterrows()\n",
        "        # cv2.imread() will convert it to a numpy array which can be used by python\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "\n",
        "        if self.augmentation is not None:\n",
        "          self.augmentation.fit(image)\n",
        "          image = self.augmentation.flow(image).next()\n",
        "\n",
        "        # target is a dictionary. Against each key it keeps a tab of how many sub-categories each category has.\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "# on_epoch_end is a must function we need to give for keras sequence class, it will change index of batch after each epoch\n",
        "# self.df.sample -> Return a random sample of items from the input df.\n",
        "# frac = 1 denotes to return whole fraction of axis items\n",
        "# reset_index(drop=True) drops the current index of the DataFrame and replaces it with an index of increasing integers\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ybL-WdltZfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "814933a7-cf11-49ed-fec6-fcab581e73b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# giving random state as 42 to avoid data leak\n",
        "# Else validation data will leak into train data & viceversa leading to high accuracies. Model will perform poor in unseen data in such cases\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15,random_state=42)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "811b8e83-10d0-4363-c4db-a11dc072c1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "train_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10416</th>\n",
              "      <td>resized/10418.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>resized/3496.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10416  resized/10418.jpg              0  ...                        0              0\n",
              "3495    resized/3496.jpg              0  ...                        0              0\n",
              "\n",
              "[2 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, \n",
        "                                augmentation=ImageDataGenerator(rescale=1./255,\n",
        "                                                                featurewise_center=True,\n",
        "                                                                featurewise_std_normalization=True,\n",
        "                                                                horizontal_flip=True,vertical_flip=False,rotation_range=90,width_shift_range=.2,\n",
        "                                                                preprocessing_function=get_random_eraser(v_l=1, v_h=1)),incl_orig=True)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False,\n",
        "                                augmentation=ImageDataGenerator(rescale=1./255,\n",
        "                                                                featurewise_center=True,\n",
        "                                                                featurewise_std_normalization=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMeuJBmRcElk",
        "colab_type": "code",
        "outputId": "7858ed36-f4e5-4619-ae72-6ff8561a28dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(3):\n",
        "  cv2_imshow(train_gen[i][0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index 0\n",
            "batch_slice slice(0, 32, None)\n",
            "items:            image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
            "0    resized/6551.jpg              1  ...                        0              0\n",
            "1    resized/5394.jpg              0  ...                        1              0\n",
            "2    resized/5292.jpg              0  ...                        0              1\n",
            "3    resized/2328.jpg              0  ...                        0              0\n",
            "4   resized/13561.jpg              0  ...                        0              1\n",
            "5   resized/11191.jpg              1  ...                        0              0\n",
            "6    resized/3746.jpg              1  ...                        0              1\n",
            "7    resized/9017.jpg              1  ...                        1              0\n",
            "8    resized/7095.jpg              0  ...                        1              0\n",
            "9    resized/8529.jpg              0  ...                        1              0\n",
            "10   resized/5914.jpg              1  ...                        1              0\n",
            "11   resized/1295.jpg              0  ...                        0              1\n",
            "12   resized/2285.jpg              0  ...                        1              0\n",
            "13  resized/11390.jpg              1  ...                        0              0\n",
            "14  resized/12784.jpg              0  ...                        0              0\n",
            "15   resized/9912.jpg              1  ...                        1              0\n",
            "16   resized/6011.jpg              1  ...                        1              0\n",
            "17   resized/1085.jpg              1  ...                        1              0\n",
            "18    resized/601.jpg              0  ...                        0              1\n",
            "19    resized/771.jpg              1  ...                        1              0\n",
            "20   resized/9233.jpg              1  ...                        1              0\n",
            "21  resized/10270.jpg              1  ...                        0              1\n",
            "22   resized/5198.jpg              1  ...                        1              0\n",
            "23   resized/6745.jpg              0  ...                        0              1\n",
            "24   resized/9260.jpg              1  ...                        1              0\n",
            "25  resized/11447.jpg              0  ...                        0              0\n",
            "26   resized/5346.jpg              1  ...                        1              0\n",
            "27   resized/8700.jpg              0  ...                        0              1\n",
            "28   resized/8132.jpg              0  ...                        0              1\n",
            "29  resized/11948.jpg              0  ...                        0              0\n",
            "30   resized/9878.jpg              0  ...                        1              0\n",
            "31   resized/2453.jpg              0  ...                        1              0\n",
            "\n",
            "[32 rows x 28 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAAqUlEQVR4nO3BMQEAAADCoPVPbQlP\noAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvgZM/gABE/clzwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FBC4BA1E390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "index 1\n",
            "batch_slice slice(32, 64, None)\n",
            "items:            image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
            "32   resized/1578.jpg              0  ...                        0              1\n",
            "33   resized/6544.jpg              0  ...                        1              0\n",
            "34   resized/7120.jpg              1  ...                        0              0\n",
            "35   resized/3800.jpg              0  ...                        1              0\n",
            "36   resized/4496.jpg              1  ...                        1              0\n",
            "37   resized/6156.jpg              1  ...                        1              0\n",
            "38   resized/9114.jpg              0  ...                        1              0\n",
            "39   resized/1630.jpg              0  ...                        1              0\n",
            "40  resized/10422.jpg              0  ...                        1              0\n",
            "41   resized/7375.jpg              0  ...                        1              0\n",
            "42   resized/7115.jpg              0  ...                        0              0\n",
            "43  resized/12105.jpg              0  ...                        1              0\n",
            "44    resized/617.jpg              0  ...                        0              1\n",
            "45  resized/13335.jpg              1  ...                        1              0\n",
            "46  resized/11483.jpg              0  ...                        1              0\n",
            "47   resized/3907.jpg              1  ...                        1              0\n",
            "48  resized/13104.jpg              0  ...                        1              0\n",
            "49   resized/1703.jpg              0  ...                        0              1\n",
            "50   resized/1495.jpg              1  ...                        1              0\n",
            "51   resized/5377.jpg              0  ...                        1              0\n",
            "52   resized/3785.jpg              0  ...                        0              0\n",
            "53   resized/4894.jpg              0  ...                        1              0\n",
            "54   resized/6515.jpg              0  ...                        1              0\n",
            "55   resized/6447.jpg              1  ...                        1              0\n",
            "56   resized/4120.jpg              0  ...                        1              0\n",
            "57    resized/497.jpg              0  ...                        1              0\n",
            "58   resized/4900.jpg              1  ...                        0              1\n",
            "59   resized/2385.jpg              1  ...                        0              0\n",
            "60   resized/8386.jpg              1  ...                        1              0\n",
            "61   resized/9203.jpg              1  ...                        1              0\n",
            "62   resized/4145.jpg              0  ...                        0              1\n",
            "63  resized/10814.jpg              1  ...                        1              0\n",
            "\n",
            "[32 rows x 28 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAAqUlEQVR4nO3BMQEAAADCoPVPbQlP\noAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvgZM/gABE/clzwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FBC4BA37320>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "index 2\n",
            "batch_slice slice(64, 96, None)\n",
            "items:            image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
            "64   resized/2649.jpg              1  ...                        1              0\n",
            "65   resized/9985.jpg              1  ...                        1              0\n",
            "66   resized/4268.jpg              1  ...                        1              0\n",
            "67   resized/6776.jpg              0  ...                        0              1\n",
            "68   resized/5324.jpg              1  ...                        0              1\n",
            "69  resized/10894.jpg              1  ...                        0              0\n",
            "70   resized/6119.jpg              0  ...                        1              0\n",
            "71   resized/3667.jpg              1  ...                        1              0\n",
            "72   resized/9832.jpg              1  ...                        1              0\n",
            "73   resized/5584.jpg              0  ...                        1              0\n",
            "74   resized/5526.jpg              0  ...                        1              0\n",
            "75   resized/4436.jpg              1  ...                        0              1\n",
            "76  resized/11953.jpg              0  ...                        1              0\n",
            "77  resized/10336.jpg              1  ...                        1              0\n",
            "78   resized/2791.jpg              1  ...                        1              0\n",
            "79  resized/10013.jpg              0  ...                        0              1\n",
            "80   resized/9413.jpg              1  ...                        1              0\n",
            "81  resized/12277.jpg              0  ...                        1              0\n",
            "82   resized/9358.jpg              0  ...                        1              0\n",
            "83   resized/6821.jpg              0  ...                        1              0\n",
            "84  resized/12924.jpg              0  ...                        0              1\n",
            "85   resized/6604.jpg              1  ...                        1              0\n",
            "86   resized/3916.jpg              0  ...                        1              0\n",
            "87   resized/7382.jpg              0  ...                        1              0\n",
            "88   resized/5677.jpg              0  ...                        0              0\n",
            "89   resized/7543.jpg              1  ...                        0              1\n",
            "90   resized/2029.jpg              0  ...                        1              0\n",
            "91   resized/2340.jpg              0  ...                        1              0\n",
            "92   resized/1712.jpg              0  ...                        1              0\n",
            "93   resized/4173.jpg              0  ...                        0              1\n",
            "94   resized/5115.jpg              0  ...                        0              1\n",
            "95  resized/13057.jpg              0  ...                        1              0\n",
            "\n",
            "[32 rows x 28 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAAqUlEQVR4nO3BMQEAAADCoPVPbQlP\noAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvgZM/gABE/clzwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FBC4C260D68>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "2464296e-62af-41c2-c1c0-39734950e21c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "# targets is a dictionary that we get from person data generator.\n",
        "# k will get key value 'age_output' which will be split to get 'age' only\n",
        "# v will get value of disctionary. We will take how many columns are there in v to understand classes within each class. eg: v.shape[1] will be 2\n",
        "# for key 'gender_output' because there are 2 sub-categories 'male' & 'female\n",
        "# these num_units will be later used to build the final model via build_head function\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index 0\n",
            "batch_slice slice(0, 32, None)\n",
            "items:            image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
            "0    resized/6551.jpg              1  ...                        0              0\n",
            "1    resized/5394.jpg              0  ...                        1              0\n",
            "2    resized/5292.jpg              0  ...                        0              1\n",
            "3    resized/2328.jpg              0  ...                        0              0\n",
            "4   resized/13561.jpg              0  ...                        0              1\n",
            "5   resized/11191.jpg              1  ...                        0              0\n",
            "6    resized/3746.jpg              1  ...                        0              1\n",
            "7    resized/9017.jpg              1  ...                        1              0\n",
            "8    resized/7095.jpg              0  ...                        1              0\n",
            "9    resized/8529.jpg              0  ...                        1              0\n",
            "10   resized/5914.jpg              1  ...                        1              0\n",
            "11   resized/1295.jpg              0  ...                        0              1\n",
            "12   resized/2285.jpg              0  ...                        1              0\n",
            "13  resized/11390.jpg              1  ...                        0              0\n",
            "14  resized/12784.jpg              0  ...                        0              0\n",
            "15   resized/9912.jpg              1  ...                        1              0\n",
            "16   resized/6011.jpg              1  ...                        1              0\n",
            "17   resized/1085.jpg              1  ...                        1              0\n",
            "18    resized/601.jpg              0  ...                        0              1\n",
            "19    resized/771.jpg              1  ...                        1              0\n",
            "20   resized/9233.jpg              1  ...                        1              0\n",
            "21  resized/10270.jpg              1  ...                        0              1\n",
            "22   resized/5198.jpg              1  ...                        1              0\n",
            "23   resized/6745.jpg              0  ...                        0              1\n",
            "24   resized/9260.jpg              1  ...                        1              0\n",
            "25  resized/11447.jpg              0  ...                        0              0\n",
            "26   resized/5346.jpg              1  ...                        1              0\n",
            "27   resized/8700.jpg              0  ...                        0              1\n",
            "28   resized/8132.jpg              0  ...                        0              1\n",
            "29  resized/11948.jpg              0  ...                        0              0\n",
            "30   resized/9878.jpg              0  ...                        1              0\n",
            "31   resized/2453.jpg              0  ...                        1              0\n",
            "\n",
            "[32 rows x 28 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuJN10vP9bYX",
        "colab_type": "code",
        "outputId": "12504740-890f-4090-d7be-dab24fb3ea97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "one_hot_df.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "1  resized/2.jpg              1  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "\n",
              "[3 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLiD4Kqe9kJ1",
        "colab_type": "code",
        "outputId": "dada6d01-982f-4acd-9bce-a431da19a922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(one_hot_df[one_hot_df['age_35-45'] == 1].shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEeo-DwpDBZv",
        "colab_type": "code",
        "outputId": "180f66d3-1093-452d-c0c9-ba51d842eaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "col_names = list(one_hot_df.columns.values)\n",
        "len(col_names)\n",
        "i = 0\n",
        "for i in range(len(col_names)):\n",
        "    print(col_names[i],one_hot_df[one_hot_df[col_names[i]] == 1].shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_path 0\n",
            "gender_female 5937\n",
            "gender_male 7636\n",
            "imagequality_Average 7509\n",
            "imagequality_Bad 2240\n",
            "imagequality_Good 3824\n",
            "age_15-25 2494\n",
            "age_25-35 5411\n",
            "age_35-45 3435\n",
            "age_45-55 1490\n",
            "age_55+ 743\n",
            "weight_normal-healthy 8628\n",
            "weight_over-weight 891\n",
            "weight_slightly-overweight 3196\n",
            "weight_underweight 858\n",
            "carryingbag_Daily/Office/Work Bag 4603\n",
            "carryingbag_Grocery/Home/Plastic Bag 1321\n",
            "carryingbag_None 7649\n",
            "footwear_CantSee 5028\n",
            "footwear_Fancy 2507\n",
            "footwear_Normal 6038\n",
            "emotion_Angry/Serious 1500\n",
            "emotion_Happy 1609\n",
            "emotion_Neutral 9660\n",
            "emotion_Sad 804\n",
            "bodypose_Back 2207\n",
            "bodypose_Front-Frontish 8383\n",
            "bodypose_Side 2983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iucixj48GMKO",
        "colab_type": "code",
        "outputId": "52e1cdbc-1f3c-4463-a861-38814fa64774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "# To check if images are getting read correctly\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('resized/9213.jpg',0)\n",
        "cv2_imshow(img)\n",
        "print('type::',type(img))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAAurElEQVR4nM29a4xl13Um9q219j7n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\nd5XIX/wI/r9/9//r+X8B3uwFo3MLGAMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=224x224 at 0x7FBCC642CE10>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "type:: <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "27646900-d5ac-4c09-85ab-99157a402162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "backbone = ResNet50(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGP0XsQynHwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neck = backbone.output  #backbone.output gives the last layer already build by VGG16 on the above statement (block5_maxpool)\n",
        "neck = Flatten(name=\"flattener\")(neck)  # Adds Flatten layer\n",
        "neck = Dense(512, activation=\"relu\")(neck) # Adds fully connected layer with Relu Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9bAyT4enKf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tower(in_layer,dp1,dense1,dp2,dense2,act2=None):\n",
        "    head = Dropout(dp1)(in_layer)\n",
        "    head = Dense(dense1, activation=\"relu\")(head)\n",
        "    head = Dropout(dp2)(head)\n",
        "    head = Dense(dense2, activation=act2)(head)\n",
        "    return head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGBWltzD85nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tower02(in_layer,dp1,dense1,act2=None):\n",
        "    head = Dropout(dp1)(in_layer)\n",
        "    head = Dense(dense1, activation=act2)(head)\n",
        "    return head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSH1Z5wbnOQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_head(name, activator,in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=activator, name=f\"{name}_output\"\n",
        "    )(in_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH3x4hu_nRfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# heads\n",
        "# Calls build_tower function with layers till previous point i.e. in_layer\n",
        "# For each multi-label class, build_tower adds to in_layer following - dropout -> fully connected layer -> droput -> fully connected layer &  \n",
        "# returns back to build_head.\n",
        "# Build_head applies a dense layer with softmax/sigmoid activation on top of this to give final output for that particular class. eg: gender, age etc.\n",
        "# 'M odel' API will use one 'neck' and connect one 'backbone'(VGG16) with 8 'heads'(below) which are built using build_head & build_tower\n",
        "\n",
        "# gender = build_head(\"gender\", \"sigmoid\", build_tower(neck))  #gender is binary classification\n",
        "# image_quality = build_head(\"image_quality\", \"softmax\",build_tower(neck)) # image is multi-class single label, can be either good, avg or bad\n",
        "# age = build_head(\"age\", \"softmax\",build_tower(neck))  # Age is regression to arbitrary value but here we are didving to groups like 15-25, 25-35, hence multi-class single label\n",
        "# weight = build_head(\"weight\", \"softmax\",build_tower(neck)) # weight is multi-class, single label as it can healthy, over or under\n",
        "# bag = build_head(\"bag\", \"softmax\",build_tower(neck))  # bag is multi-class, single label \n",
        "# footwear = build_head(\"footwear\",\"softmax\", build_tower(neck))  # multi-class, single label \n",
        "# emotion = build_head(\"emotion\", \"softmax\", build_tower(neck))  # multi-class, single label \n",
        "# pose = build_head(\"pose\", \"softmax\",build_tower(neck)) # multi-class, single label \n",
        "\n",
        "gender = build_head(\"gender\", \"softmax\", build_tower(neck,dp1=0.2,dense1=128,dp2=0.3,dense2=128,act2='relu'))  #gender is binary classification\n",
        "image_quality = build_head(\"image_quality\", \"sigmoid\",build_tower(neck,dp1=0.2,dense1=256,dp2=0.1,dense2=128)) # image is multi-class single label, can be either good, avg or bad\n",
        "age = build_head(\"age\", \"sigmoid\",build_tower(neck,dp1=0.2,dense1=256,dp2=0.1,dense2=128))  # Age is regression to arbitrary value but here we are didving to groups like 15-25, 25-35, hence multi-class single label\n",
        "weight = build_head(\"weight\", \"sigmoid\",build_tower(neck,dp1=0.2,dense1=256,dp2=0.1,dense2=128)) # weight is multi-class, single label as it can healthy, over or under\n",
        "bag = build_head(\"bag\", \"sigmoid\",build_tower(neck,dp1=0.2,dense1=256,dp2=0.1,dense2=128))  # bag is multi-class, single label \n",
        "footwear = build_head(\"footwear\",\"softmax\", build_tower(neck,dp1=0.2,dense1=256,dp2=0.1,dense2=128))  # multi-class, single label \n",
        "emotion = build_head(\"emotion\", \"sigmoid\", build_tower02(neck,dp1=0.2,dense1=256))  # multi-class, single label \n",
        "pose = build_head(\"pose\", \"sigmoid\",build_tower(neck,dp1=0.2,dense1=256,dp2=0.1,dense2=128)) # multi-class, single label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjtxfJMDnUXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "#for layer in backbone.layers:\n",
        "#\tprint(layer)\n",
        "#\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XXgNIsENvmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True,to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.004 * 1/(1 + 0.319 * epoch), 10)\n",
        "opt = SGD(lr=0.003, momentum=0.9)\n",
        "#opt = SGD(momentum=0.5)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "#   loss = \"binary_crossentropy\",\n",
        "    loss={\"gender_output\":\"categorical_crossentropy\", \n",
        "          \"image_quality_output\":\"binary_crossentropy\",\n",
        "          \"age_output\":\"binary_crossentropy\",\n",
        "          \"weight_output\":\"binary_crossentropy\",\n",
        "          \"bag_output\":\"binary_crossentropy\",\n",
        "          \"emotion_output\": \"binary_crossentropy\",\n",
        "          \"pose_output\": \"binary_crossentropy\",\n",
        "          \"footwear_output\": \"binary_crossentropy\"},\n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "a9d8aefb-0fd1-4101-9efc-f01526cb5e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=2,\n",
        "    verbose=1)\n",
        "#    callbacks=[LearningRateScheduler(scheduler,verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "359/360 [============================>.] - ETA: 0s - loss: 37.9462 - gender_output_loss: 7.0222 - image_quality_output_loss: 5.3727 - age_output_loss: 3.2236 - weight_output_loss: 2.9359 - bag_output_loss: 7.0849 - footwear_output_loss: 5.9072 - pose_output_loss: 4.0987 - emotion_output_loss: 2.3010 - gender_output_acc: 0.5643 - image_quality_output_acc: 0.6667 - age_output_acc: 0.8000 - weight_output_acc: 0.8169 - bag_output_acc: 0.5580 - footwear_output_acc: 0.6315 - pose_output_acc: 0.7443 - emotion_output_acc: 0.8565\n",
            "\n",
            "360/360 [==============================] - 212s 589ms/step - loss: 37.9336 - gender_output_loss: 7.0167 - image_quality_output_loss: 5.3727 - age_output_loss: 3.2236 - weight_output_loss: 2.9354 - bag_output_loss: 7.0847 - footwear_output_loss: 5.9047 - pose_output_loss: 4.0957 - emotion_output_loss: 2.3002 - gender_output_acc: 0.5647 - image_quality_output_acc: 0.6667 - age_output_acc: 0.8000 - weight_output_acc: 0.8169 - bag_output_acc: 0.5580 - footwear_output_acc: 0.6317 - pose_output_acc: 0.7445 - emotion_output_acc: 0.8565 - val_loss: 38.0934 - val_gender_output_loss: 7.2276 - val_image_quality_output_loss: 5.3727 - val_age_output_loss: 3.2236 - val_weight_output_loss: 2.8347 - val_bag_output_loss: 6.9390 - val_footwear_output_loss: 6.0697 - val_pose_output_loss: 4.0765 - val_emotion_output_loss: 2.3497 - val_gender_output_acc: 0.5516 - val_image_quality_output_acc: 0.6667 - val_age_output_acc: 0.8000 - val_weight_output_acc: 0.8232 - val_bag_output_acc: 0.5671 - val_footwear_output_acc: 0.6214 - val_pose_output_acc: 0.7457 - val_emotion_output_acc: 0.8534\n",
            "Epoch 2/2\n",
            "359/360 [============================>.] - ETA: 0s - loss: 37.9368 - gender_output_loss: 7.0180 - image_quality_output_loss: 5.3727 - age_output_loss: 3.2236 - weight_output_loss: 2.9345 - bag_output_loss: 7.0867 - footwear_output_loss: 5.9053 - pose_output_loss: 4.0950 - emotion_output_loss: 2.3010 - gender_output_acc: 0.5646 - image_quality_output_acc: 0.6667 - age_output_acc: 0.8000 - weight_output_acc: 0.8169 - bag_output_acc: 0.5579 - footwear_output_acc: 0.6316 - pose_output_acc: 0.7445 - emotion_output_acc: 0.8565"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM6n4pEYO0fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H72RHXbdOzmM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "outputId": "4176d68b-eb9a-42c5-c9a5-56cdbd453559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f36987ad860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}